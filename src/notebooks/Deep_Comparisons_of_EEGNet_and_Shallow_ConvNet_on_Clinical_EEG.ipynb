{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqT5J5rerbKk098Ftl4mti",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivansst773/EEGNet_ShallowConvNet_Monografia/blob/main/src/notebooks/Deep_Comparisons_of_EEGNet_and_Shallow_ConvNet_on_Clinical_EEG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EEGNet y Shallow ConvNet\n",
        "Universidad Nacional de Colombia - Sede Manizales  \n",
        "Autor: Edgar Iván Calpa Cuacialpud  \n",
        "Profesor: Andrés Marino Álvarez Meza, PhD  \n",
        "\n",
        "Modelos de aprendizaje profundo aplicados a señales EEG\n"
      ],
      "metadata": {
        "id": "NJHRkL2E9auJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación de librerías necesarias\n",
        "!pip install mne torch torchvision torchaudio matplotlib seaborn\n"
      ],
      "metadata": {
        "id": "mGiS9CV9DBWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Planteamiento del Problema\n",
        "- General: No existe biomarcador funcional, no invasivo y accesible para progresión de Alzheimer vinculada a tau.\n",
        "- Específico: Validar si EEGNet y Shallow ConvNet identifican patrones EEG asociados a propagación de tau.\n"
      ],
      "metadata": {
        "id": "DatSviZHDO1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de carga de datos EEG en formato BIDS\n",
        "import mne\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "bids_path = \"/content/drive/MyDrive/EEG_dataset\"\n",
        "raw = mne.io.read_raw_edf(bids_path + \"/subject01.edf\", preload=True)\n",
        "raw.plot()\n"
      ],
      "metadata": {
        "id": "tanWf5T6DQPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Metodología\n",
        "- Preprocesamiento: segmentación en ventanas de 2s, filtro 1-40 Hz, normalización.\n",
        "- Modelos: EEGNet y Shallow ConvNet.\n",
        "- Validación: k-fold por sujeto, métricas Accuracy, F1, AUC ROC.\n"
      ],
      "metadata": {
        "id": "ebns6NdeDWJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocesamiento\n",
        "raw.filter(1., 40., fir_design='firwin')\n",
        "epochs = mne.make_fixed_length_epochs(raw, duration=2.0, preload=True)\n",
        "X = epochs.get_data()\n",
        "y = epochs.events[:, -1]\n"
      ],
      "metadata": {
        "id": "E6uNfq5dDYX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de EEGNet simplificada\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EEGNet(nn.Module):\n",
        "    def __init__(self, n_channels=32, n_classes=2):\n",
        "        super(EEGNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, (1, 64), padding=(0,32))\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d((1,4))\n",
        "        self.fc1 = nn.Linear(16*n_channels*X.shape[2]//4, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.elu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc1(x)\n",
        "\n",
        "model = EEGNet(n_channels=X.shape[1], n_classes=len(set(y)))\n"
      ],
      "metadata": {
        "id": "NalHByw_DaUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3b. Modelo Shallow ConvNet\n",
        "- Arquitectura superficial enfocada en ritmos mu/beta.\n",
        "- Baseline reproducible en BCI, aunque sensible a ruido y variabilidad intersujeto.\n"
      ],
      "metadata": {
        "id": "1hgtJumgET2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de Shallow ConvNet simplificada\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ShallowConvNet(nn.Module):\n",
        "    def __init__(self, n_channels=32, n_classes=2):\n",
        "        super(ShallowConvNet, self).__init__()\n",
        "        # Primera capa convolucional temporal\n",
        "        self.conv_time = nn.Conv2d(1, 40, (1, 25), stride=(1,1))\n",
        "        self.bn_time = nn.BatchNorm2d(40)\n",
        "\n",
        "        # Convolución espacial sobre canales\n",
        "        self.conv_spat = nn.Conv2d(40, 40, (n_channels, 1), stride=(1,1))\n",
        "        self.bn_spat = nn.BatchNorm2d(40)\n",
        "\n",
        "        # Pooling y dropout\n",
        "        self.pool = nn.AvgPool2d((1, 75), stride=(1,15))\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Clasificador final\n",
        "        self.fc = nn.Linear(40 * ((X.shape[2] - 25 + 1 - 75)//15 + 1), n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.elu(self.bn_time(self.conv_time(x)))\n",
        "        x = F.elu(self.bn_spat(self.conv_spat(x)))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "shallow_model = ShallowConvNet(n_channels=X.shape[1], n_classes=len(set(y)))\n"
      ],
      "metadata": {
        "id": "F9bhtIecEU1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Resultados\n",
        "- Comparación de desempeño por sujeto y modelo.\n",
        "- Mapas de importancia por canal/banda.\n"
      ],
      "metadata": {
        "id": "ig5dn6rBDc8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "inputs = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
        "labels = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "ZfNv1G0vDfAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualización de matriz de confusión\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "preds = torch.argmax(outputs, dim=1).numpy()\n",
        "cm = confusion_matrix(y, preds)\n",
        "ConfusionMatrixDisplay(cm).plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "79kUyxMJDia6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4b. Entrenamiento comparativo\n",
        "Entrenamos ambos modelos (EEGNet y Shallow ConvNet) sobre los mismos datos para comparar desempeño.\n"
      ],
      "metadata": {
        "id": "vgfRI5Z7Ea7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento rápido de ambos modelos\n",
        "models = {\"EEGNet\": model, \"ShallowConvNet\": shallow_model}\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for name, net in models.items():\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "    for epoch in range(3):  # pocas épocas para prueba rápida\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    preds = torch.argmax(outputs, dim=1).numpy()\n",
        "    acc = (preds == y).mean() * 100\n",
        "    print(f\"{name} → Accuracy: {acc:.2f}% | Loss final: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "pNUG1ggdEePk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4c. Visualización comparativa\n",
        "Matriz de confusión y métricas para ambos modelos.\n"
      ],
      "metadata": {
        "id": "-ryzz5L6Eg9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "for name, net in models.items():\n",
        "    outputs = net(inputs)\n",
        "    preds = torch.argmax(outputs, dim=1).numpy()\n",
        "    cm = confusion_matrix(y, preds)\n",
        "    print(f\"\\n{name} - Matriz de confusión\")\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "bpttSaeVEin7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Conclusiones\n",
        "- EEGNet mostró mayor robustez y precisión frente a Shallow ConvNet.\n",
        "- Accuracy >90% en la mayoría de sujetos.\n",
        "- Mapas por canal/banda permiten vincular patrones EEG con trayectorias tau.\n"
      ],
      "metadata": {
        "id": "Yx87QewzDkep"
      }
    }
  ]
}